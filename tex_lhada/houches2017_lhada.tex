\documentclass[11pt]{cernrep}
\usepackage{graphicx,epsfig}
\usepackage{amsmath}
\usepackage[T1]{fontenc}
\usepackage{fancyvrb}
\bibliographystyle{lesHouches}
\begin{document}


\author{Philippe~Gras$^{1}$, Sezen~Sekmen$^{2}$}

\institute{
  $^{1}$ IRFU, CEA, Universit\'e Paris-Saclay, Gif-sur-Yvette\\
  $^{2}$ Kyungpook Univ., Daegu
}


\title{Analysis description for LHC result reinterpretations}

\maketitle

\abstract{{\sc Lhada} is a language to describe LHC analysis which has a wide range of applications. In this work, this language is investigated for its usage in the context of LHC result reinterpretation. It would be employed to describe in an unambiguous and concise manner a data analysis including all the details needed for a reinterpretation of the result in the context of a physics theory not considered in the original analysis. A specialisation of the language dedicated to reinterpretation is introduced. The specialisation defines extra syntax rules and constitutes a subset of the language. Three different analyses used as benchmarks are described with this language. Automatic generation of code reproducing the analysis on Monte-Carlo samples for the purpose of result reinterpretation is investigated. We demonstrate that programs that generates code to be used in a result reinterpretation tool can be easily developed and a prototype is presented. In addition, the generated code can be used to validate the accuracy of the analysis description.}

\section{Introduction}

The need for a standard to describe analyses of LHC data in an unambiguous way together with the definition of its requirements has been studied at the 2015 session of Les Houches PhysTeV workshop~\cite{Brooijmans:2016vro}. The study includes a proposal for this standard. Other choices are possible, for instance {\sc CutLand}~\cite{Sekmen:2018ehb} is another description language that fulfils similar requirements. In this work, we are investigating the standard proposed in Les Houches proceedings in the context of analysis reinterpretation. For convenience we will call this standard {\sc Lhada}, although it's one implementation of Les Houches analysis description accord (Lhada), which refers to the language requirement only. Three questions are addressed: the coverage of the language, that is its ability to implement a large spectrum of analyses, the completeness of the analysis description, and the capacity to validate this description. The first question is addressed by implementing the description of example analyses with different levels of complexity. The two others were addressed by developing two machine interpreters. The interpreters generate c$++$ code that reproduces the analysis on an input samples. One interpreter produces a module, so-called ``Rivet analysis'', for the Rivet~\cite{Buckley:2010ar} framework, while the second produces a standalone code based on the ROOT~\cite{Brun:1997pa} framework. The Rivet based code is meant to be used for result reinterpretation. It can also be used to validate an analysis description by reproducing reference numbers provided by the analysis authors. In particular the cut flow, that is the acceptances of the subsequent selections (the ``cuts'') of the analysis, is well suited for such validation~\cite{Kraml:2012sg}. The completeness of the description is validated at the same time.

\section{Describing analyses  {\sc Lhada}}

In order to test the suitability of the {\sc Lhada} language to describe a LHC analysis three different analyses have been considered. The first analysis is the {\em Search for new physics in the all-hadronic final state with the MT2 variable} from Ref.~\cite{CMS:2016xva}. The two other analyses are the {\em Search for squarks and gluinos in final states with jets and missing transverse momentum at $\sqrt{s} = 13\,$TeV  with the ATLAS detector}~\cite{Aaboud:2016zdn} and {\em Search for dark matter at $\sqrt{s}=13\,$TeV in final states containing an energetic photon and large missing transverse momentum with the ATLAS detector}~\cite{Aaboud:2017dor}, which are used for the comparison of the reinterpretation tools performed in Contribution~\verb|\ref{sec:recast}|.

We have described the three analyses with the {\sc Lhada} language and the descriptions can be found in the analysis descriptions database~\cite{bib:lhada_git} respectively under {\tt lhada/analyses/CMS-PAS-SUS-16-015}, {\tt lhada/analyses /ATLASSUSY1605.03814}, and {\tt lhada/analyses/ATLASEXOT1704. 0384}. No particular difficulty has been encountered. The {\sc Lhada17} language subset has been used and a cut flow table has been included [editor's note: to be added] in the description to allow the validation of the descriptions using code generated with the interpreter.


\section{Generating a Rivet analysis from Lhada}

{\sc Lhada} is a multipurpose and flexible language. In the case of {\sc Lhada2rivet}, the interpreter that generates a {\sc Rivet} analysis, we have chosen to limit ourselves to the analysis reinterpretation use case and to specify accurately the analysis description language understood by the program. For this purpose, we have derived a subset of the {\sc Lhada} language, called {\sc Lhada17}. In this section, we will first draw the specifications of this sublanguage. We will then present the automatic generation of Rivet analyses.

\subsection{Describing the description language}\label{sec:desc}

%Machine interpretation of the analysis descriptions requires a precise specification of the language. While the level of specification given in Ref.~\cite{Brooijmans:2016vro} is well suited for human interpretation, implementing a parser that support all its flexible is a difficult task. Unnecessary software complexity is required to handle all possible ways to describe the analyses and it is difficult to anticipate all possible cases allowed by the language. In this section we refine these specifications for the purpose of machine interpretation in the context of result reinterpretation. The resulting specifications, we will call {\sc Lhada}17, can be seen as a subset of the {\sc Lhada} language.

The Extended Backus-Naur Form~\cite{bib:ebnf,bib:ebnf-wiki} (EBNF) notation has been used to specify the syntax and grammar of {\sc Lhada}17. The syntax is given in Appendix~\ref{app:ebnf}. The following rules which have not been included in the EBNF syntax to simplify the notation apply:
\begin{itemize}
\item A hash sign (\#) can be used to include comments in the {\sc Lhada} files: all characters of a line starting from a hash sign are comments and ignored for the interpretation based on the EBNF description.
\item If the last non-space character of a line is a backslash ($\backslash$), then the line is merged with the following line before being interpreted according to the EBNF description.
\item An entity ({\tt function}, {\tt object}  or {\tt cut}) should be declared before being used. For instance if a function is used in a ``cut'' definition, the corresponding {\tt function} block should appear before the {\tt cut} line. This rule is meant to simplify the parsing and also to avoid circular definitions. 
\end{itemize}

A specificity of {\sc Lhada} is the usage of programming languages to describe algorithms, via the {\sc Lhada} {\tt functions} while the main structure of the analysis is described with the dedicated language. A reference implementation of the algorithm is provided in a ``commonly used'' programming language. The implementation is given in a code source file, which can group implementations of different {\sc Lhada} {\tt functions} and can be shipped along with the {\sc Lhada} description file or provided as an http link.  In order to ease machine interpretation {\sc Lhada}17 includes the following restrictions for the reference {\tt function} implementations.

\begin{itemize}
\item the implementation is written in c$++$11~\cite{bib:c++11};
\item the implementation must depend only on the code provided in the file and libraries from the restricted set defined below; the file should be compilable with c$++$11 compliant compilers.
\item the allowed types for the function parameters are: {\tt int}, {\tt float}, {\tt double}, {\tt std::string}, {\tt LHADAParticle}, and {\tt FourMomentum}. Parameters are passed by copy (no modifier) or by constant reference (with {\tt const \&} modifier, like {\tt const LHADAParticle\&}); this rules exclude the use of a templated function; function templates are allowed for auxiliary functions; 
\item {\tt \#include} statements can be used to include header files from the allowed libraries;
\item the file, where the functions is defined should be compilable with c$++$11 compliant compilers;
\item self-contained function are encouraged but not mandatory; by self contained we mean that the file is compilable with c$++$11 compliant compilers after having removed all code except the function and the {\tt \#include} that precedes it;
\item the function can use the random object to draw pseudo-random numbers, whose scope is global to all functions and which provides the methods described in Table~\ref{tab:rand}; 
\end{itemize}

The {\tt LHADAParticle} and {\tt Momentum} types are two classes which provides the methods respectively described in Tables~\ref{tab:part} and \ref{tab:mom}. The restricted set of libraries includes the libraries that comes with the c$++$ standard ({\tt std} libraries) and a common library provided in the {\sc Lhada} repository. The header file coming with the latest library will be callded {\tt lhadatools-vXX.h} where {\tt XX} is a string specifying the library version. The set of libraries can evolve without requiring a revision of the {\sc Lhada17} language standard and will be defined by a list stored in the {\sc Lhada} repository.

\begin{table}
  \caption{Definition of the LHADAParticle type. \label{tab:part}}
  \begin{tabular}{l|l|l}
    Name of the property in the LHADA file & c$++$ method & Description \\
    \hline
    mass        & mass() & Mass\\
    e           & e()    & Energy\\
    px          & px()   & momentum x-component\\
    py          & py()   & momentum y-component\\
    pz          & pz()   & momentum z-component\\
    pt          & pt()   & absolute transverse momentum\\
    eta         & eta()  & pseudorapidity\\
    rapidity    & rapidity() & rapidity\\
    charge      & charge() & charge\\
    spin        & spin() & spin\\
    pdgid       & pdgid() & PDG particle id\\
    phi         & phi()   & momentum phi coordinate\\
    x           & x()     & particle production vertex x-coordinate\\
    y           & y()     & particle production vertex y-coordinate\\
    z           & z()     & particle production vertex z-coordinate  \\
  \end{tabular}
\end{table}


\begin{table}
  \caption{Definition of the FourMomentum type. \label{tab:mom}}
  \begin{tabular}{l|l|l}
    Name of the property in the LHADA file & c$++$ method & Description \\
    \hline
    mass 	& mass()     & Mass\\
    e 	        & e()        & Energy\\
    px 	        & px()       & momentum x-component\\
    py 	        & py()       & momentum y-component\\
    pz 	        & pz()       & momentum z-component\\
    pt 	        & pt()       & absolute transverse momentum\\
    eta         & eta()      & pseudorapidity\\
    rapidity    & rapidity() & rapidity\\
  \end{tabular}
\end{table}

 
\begin{table}
  \caption{Definition of the random object interface: list of provided methods. \label{tab:rand}}
  \begin{tabular}{l|p{20em}}
    c$++$ method & Description \\
    \hline
    uniform(double x)  & Returns a pseudorandom number following a uniform distribution over the $[0, \text{x}]$ interval.\\
    gauss(double mean, double sigma)    & Returns a pseudorandom number following a Gaussian distribution.\\
    poisson(int mean) & Returns a pseudorandom number following a Poisson distribution.\\
    breitWigner(double mean, double gamma) & Returns a pseudorandom number following a Breit-Wigner distribution. \\
    exp(double tau) & Return a pseudorandom number following the $\exp(-t/\text{tau})$ distribution. \\
    landau(double mean, double sigma) & Returns a pseudorandom number following a Landau distribution. \\
    binomial(int ntot, double prob) & Returns a pseudorandom number in the $[0, \text{ntot}]$ interval following a Binomial distribution.
  \end{tabular}
\end{table}

%%\#include ``lhada17v1.h''?

The {\sc LHADA} language does not explicitly specify how the arguments listed in a {\tt function} block are matched to the arguments of the reference implementation of the function. To prevent confusion, {\sc LHADA17} requires that the arguments appear in the {\tt function} block in the order of the c$++$ function argument list of its reference implementation and with the same name. If the names differ, the arguments should be matched according to their order, though in such case the file can simply be considered as invalid.

An {\tt object} block defines an entity, typically a collection of particles, starting from the input defined by the {\tt take} statement that is transformed by a sequence of {\tt apply}, {\tt select}, and {\tt reject}  statements. The {\tt apply} statement specifies a function that transforms the entity. In {\sc Lhada17}, the function must take as first argument the entity to transform. This argument is specified in the function definition, but not on the {\tt apply} statement, where it is implicit. Collections are filtered with {\tt select} and {\tt reject} statements. A condition to respectively keep or reject a collection element is provided in the form of a boolean expression. In addition to arithmetic and boolean operations, the expression can contain calls to functions. In the examples given in Ref.~\cite{Brooijmans:2016vro}, the functions take the collection element as an implicit argument, but this might not always be the case. In {\sc Lhada17} the following rule applies: if the number of passed arguments is less by one with respect to the expected one, then the collection element to filter is assumed to be implicitly passed as first argument. A mismatch between the expected argument type and the collection element type is considered as ill-formed. While the {\tt apply} statement is valid for an entity which is a single object (e.g. missing transverse momentum), the {\tt select} and {\tt reject} statements are restricted to collections. Blocks without a {\tt take} statement are also allowed. In this case there is no implicit argument in the {\tt apply} statement. It is strongly discourage to use this form when the one with a {\tt take} statement can be used.

Three extensions to {\sc LHADA} are introduced in {\sc LHADA17}. The first is the backslash line continuation marker described above. The second extension is the {\tt uid} attribute of the {\tt object} block that is defined in the EBNF language description. This attribute, whose name stands for ``universal identifier'', identifies the {\tt object}. In {\sc Lhada17}, object blocks with a {\tt take external} statement include a {\tt uid} and no {\tt apply} or {\tt cut} statement. The {\tt uid} attribute is reserved to this type of {\tt objects}. Finally, we have introduced two aliases for the keyword {\tt object}: {\tt variable} and {\tt collection}. The {\tt object} block can represent several types of entities. Providing the possibility to use a name reflecting the type of entities, {\tt collection} when dealing with a collection of physics object, {\tt variable} when dealing with a single observable, like an event shape variable, should help in writing more intelligible analysis descriptions. The choice of the name is left to the discretion of the analysis description author.

\subsection{Automatised generation of Rivet analysis code}

The {\sc Lhada} language will play a role for LHC result reinterpretations only if it is interfaced to commonly used reinterpretation frameworks. The interface can be done in two different ways. The first approach is to interpret the analysis description at run time. The second one, which is adopted here, is to generate code from the description.

A prototype application, called {\sc Lhada2Rivet}, that produces a {\sc Rivet} analysis from its description in {\sc Lhada17} has been developed. The analysis produces a cut flow table using. Special care has been taken to produce code in the {\sc Rivet} style using facilities provided by the framework, like the projections or the {\tt CutFlow} class.

Detector response simulation has been partly addressed. The random object was introduced in {\sc Lhada17} to allow the definition of  the detector resolution and efficiency using a c$++$ function.  Such functions will be added to the central database along with the defined objects. In addition, the object {\tt uid} provides the identification of the objects whose detector response is already included in {\sc Rivet} permitting to use the Rivet built-in detector response simulation in such case. Support for detector response simulation is expected to be included soon in the {\sc Lhada2Rivet} interpreter based on these features.

The code produced by the {\sc Lhada2Rivet} interpreter for the first analysis considered in the previous section can be found in appendix~\ref{app:code}. Each {\sc Lhada} cut block is mapped to a c$++$ method. Rivet built-in tools, as Projections and Cutflow are used, leading to a clean code that is well integrated in the framework. The {\sc Rivet} interface to the fastjet~\cite{Cacciari:2011ma} library is used. The anti-$k_{\text{T}}$ jet used in the analysis is identified as such by the interpreter based on the name of the function specified in the {\tt apply} statement of the {\sc Lhada} {\tt object} block. This identification based on a name convention will be replaced by use the common library introduced in Sec.~\ref{sec:desc} in future update of the interpreter.

With this prototype we have investigated the different aspects that a result reinterpretation code generator based on a {\sc Lhada} analysis description should cover. We can conclude from this exercise that the development of such a generator that takes as input a description compliant with the {\sc Lhada17} specifications can be done with a limited effort. The code produced by such a generator can be used to validate the analysis description using analysis cut flow, which needs to be provided by the analysis authors. 


\section{The {\sc Lhada2TNM} interpreter}
%
%{\em placeholder for Harrison's contribution.}

Two key design features of {\sc Lhada} are human readability and analysis framework independence. As noted above, framework independence can be tested by attempting to implement tools that automatically translate analyses described using {\sc Lhada} into analyses that can be executed in different analysis frameworks. Human readability demands that  the number of rules and syntactical elements be kept as low as possible. But, since we also demand that {\sc Lhada} be sufficiently expressive to capture the details of LHC analyses, it pays to follow Einstein's advice: ``Everything should be made as simple as possible, but not simpler".    In the prototype of the  {\sc Lhada2TNM} translator, we have tried to place the burden where it properly belongs, namely, on the translator. For example, it is expected that physicists will write {\sc Lhada} files so that blocks appear in a natural order. However, the {\sc Lhada2TNM} translator does not rely on the order of blocks within a {\sc Lhada} file. The appropriate ordering of blocks is handled by the translator. Given a {\tt cut} block called {\tt signal}, which makes use of another called {\tt preselection}, {\sc Lhada2TNM} places  the code for {\tt preselection} before the code for {\tt signal} in the resulting C++ file.

Another example of placing the burden on the translator rather than on the author of a {\sc Lhada} file, concerns statements that span multiple lines. Many computer languages have syntactical elements to identify such statements. However, since {\sc Lhada} is a keyword-value language, continuation marks are not needed because it is possible to identify when the value associated with a statement ends.

The {\sc Lhada2TNM} translator is a {\tt Python} program that translates a {\sc Lhada} file to a C++ program that can be executed within the {\sc TNM} $n$-tuple analysis framework. The translator extracts all the blocks from a {\sc Lhada} file and places them within a data structure that groups the blocks according to type. The {\tt object} and {\tt cut} blocks are ordered according to their dependencies on other object or cut blocks. It is assumed that a standard, extensible, type is available to model all analysis objects and that an adapter exists to translate input types, e.g., {\sc Delphes}, {\sc ATLAS}, {\sc CMS}, etc., types  to the standard type. In order to determine where a statement ends, {\sc Lhada2TNM} looks ahead to the next record in the {\sc Lhada} file during translation.

In the current version, instances of the standard, extensible, type as well as functions are placed in the global namespace of the C++ program so that the object and cut code blocks
that need them can access them without the need to pass objects between code blocks. The name of a function defined in {\sc Lhada} is assumed  to be identical with that of a function, which, ultimately, will be accessed from an online code repository. However, this assumption can be relaxed if warranted in a later iteration of {\sc Lhada}; for example, the appropriate function can be specified by its {\sc DOI}.  While the technical details of the automatic access of
codes from an online repository need to be worked out, we see no insurmountable hurdles.


\section{Conclusion}

The sustainability of the {\sc Lhada} language to be used in the context of analysis reinterpretation has been studied. Three questions have been addressed: the analysis range the language can cover, the completeness of the analysis description, and the capacity to validate the analysis description. The analysis coverage question has been tackled by taking three different LHC results and implementing their description. The language turns out to be very flexible and no difficulty has been encountered in this exercise giving confidence that the coverage covers the need. Nevertheless, it will be wise to extend the exercise with more sophisticated analyses. The two other questions have been addressed by developing the prototypes of two applications that interpret analysis descriptions written in {\sc Lhada}. The development of an application generating reinterpretation code out of a {\sc Lhada} analysis description can easily be done provided that the syntax used by the description is well-defined and not too flexible. A specialisation, {\sc Lhada17}, of the {\sc Lhada} language that fulfils this requirement has been set up. The restrictions introduced by {\sc Lhada17} have not been a limitation for the description of the analysis considered in the coverage test. 

\bibliography{houches2017_lhada.bib}

\section*{Appendices}

\appendix

\section{{\sc Lhada17} language syntax}\label{app:ebnf}

\fvset{fontsize=\footnotesize}
\VerbatimInput{lhada.ebnf}

\section{Example of code produced by the {\sc Ladha2rivet} interpreter}\label{app:code}

\VerbatimInput{CMS_PAS_SUS_16_015.cc}

\end{document} 

